"""
AIåˆ†ææ¨¡å—
è´Ÿè´£è°ƒç”¨é€šä¹‰å¤§æ¨¡å‹ï¼Œåˆ†ææ–‡ä»¶å¹¶ç»™å‡ºæ•´ç†å»ºè®®

ã€é‡è¦ã€‘è¯·åœ¨æ­¤æ–‡ä»¶ä¸­å¡«å†™é€šä¹‰å¤§æ¨¡å‹çš„è°ƒç”¨ä»£ç 
"""
import json
from typing import List, Dict, Optional
import config

# TODO: å¯¼å…¥é€šä¹‰åƒé—®çš„SDK
# ç¤ºä¾‹: from dashscope import Generation
# æˆ–è€…ä½¿ç”¨ OpenAI å…¼å®¹çš„ API


class AIAnalyzer:
    """AIæ–‡ä»¶åˆ†æå™¨"""

    def __init__(self):
        """åˆå§‹åŒ–AIåˆ†æå™¨"""
        self.api_key = config.TONGYI_API_KEY
        self.model = config.TONGYI_MODEL

        # TODO: åˆå§‹åŒ–é€šä¹‰åƒé—®å®¢æˆ·ç«¯
        # ç¤ºä¾‹:
        # import dashscope
        # dashscope.api_key = self.api_key

    def analyze_files(self, files: List[Dict], progress_callback=None) -> Dict:
        """
        åˆ†ææ–‡ä»¶åˆ—è¡¨ï¼Œè¿”å›æ•´ç†å»ºè®®ï¼ˆæ”¯æŒåˆ†æ‰¹å¤„ç†ï¼‰

        :param files: æ–‡ä»¶ä¿¡æ¯åˆ—è¡¨ï¼Œæ¯ä¸ªæ–‡ä»¶åŒ…å« name, path, extension, size_mb, modified_time ç­‰å­—æ®µ
        :param progress_callback: è¿›åº¦å›è°ƒå‡½æ•°ï¼Œæ¥æ”¶ (current_batch, total_batches, batch_result) å‚æ•°
        :return: åˆ†æç»“æœï¼ŒåŒ…å«åˆ†ç±»å»ºè®®ã€åˆ é™¤å»ºè®®ç­‰

        è¿”å›æ ¼å¼ç¤ºä¾‹:
        {
            'suggestions': [
                {
                    'file_path': '/path/to/file1.txt',
                    'action': 'delete',  # delete åˆ é™¤, move ç§»åŠ¨, keep ä¿ç•™
                    'reason': 'è¿™æ˜¯ä¸€ä¸ªä¸´æ—¶æ–‡ä»¶ï¼Œå·²è¶…è¿‡30å¤©æœªä½¿ç”¨',
                    'category': 'ä¸´æ—¶æ–‡ä»¶',
                    'confidence': 0.9  # ç½®ä¿¡åº¦ 0-1
                },
                ...
            ],
            'categories': {
                'ä¸´æ—¶æ–‡ä»¶': ['file1.txt', 'file2.txt'],
                'é‡è¦æ–‡æ¡£': ['file3.docx'],
                ...
            }
        }
        """

        # åˆ†æ‰¹å¤„ç†æ–‡ä»¶
        batch_size = config.MAX_FILES_PER_REQUEST
        total_files = len(files)

        if total_files > batch_size:
            print(f"\n" + "="*80)
            print(f"ğŸ“¦ æ‰¹é‡å¤„ç†æ¨¡å¼")
            print(f"   æ€»æ–‡ä»¶æ•°: {total_files}")
            print(f"   æ¯æ‰¹å¤§å°: {batch_size} (å¯åœ¨config.pyä¸­è°ƒæ•´MAX_FILES_PER_REQUEST)")
            total_batches = (total_files + batch_size - 1) // batch_size
            print(f"   åˆ†æ‰¹æ•°é‡: {total_batches}")
            print("="*80)

            # åˆ†æ‰¹å¤„ç†å¹¶åˆå¹¶ç»“æœ
            all_suggestions = []
            all_categories = {}

            for i in range(0, total_files, batch_size):
                batch = files[i:i + batch_size]
                batch_num = i // batch_size + 1

                print(f"\n" + "â–¶"*40)
                print(f"ğŸ“‹ å¤„ç†æ‰¹æ¬¡ {batch_num}/{total_batches}")
                print(f"   æ–‡ä»¶èŒƒå›´: {i+1} - {min(i+batch_size, total_files)}")
                print(f"   æœ¬æ‰¹æ–‡ä»¶æ•°: {len(batch)}")
                print(f"   æ–‡ä»¶åˆ—è¡¨:")
                for idx, f in enumerate(batch[:5], 1):  # åªæ˜¾ç¤ºå‰5ä¸ª
                    print(f"      {idx}. {f['name']} ({f['size_mb']}MB)")
                if len(batch) > 5:
                    print(f"      ... è¿˜æœ‰ {len(batch)-5} ä¸ªæ–‡ä»¶")
                print("â–¶"*40 + "\n")

                batch_result = self._analyze_batch(batch, batch_num, total_batches)

                # ç»Ÿè®¡æœ¬æ‰¹ç»“æœ
                batch_suggestions_count = len(batch_result.get('suggestions', []))
                print(f"\nâœ”ï¸  æ‰¹æ¬¡ {batch_num} å®Œæˆï¼Œè·å¾— {batch_suggestions_count} æ¡å»ºè®®\n")

                all_suggestions.extend(batch_result.get('suggestions', []))

                # åˆå¹¶åˆ†ç±»
                for category, file_list in batch_result.get('categories', {}).items():
                    if category in all_categories:
                        all_categories[category].extend(file_list)
                    else:
                        all_categories[category] = file_list

                # è°ƒç”¨è¿›åº¦å›è°ƒï¼Œå®æ—¶æ›´æ–°GUI
                if progress_callback:
                    progress_callback(batch_num, total_batches, batch_result)

            print(f"\n" + "="*80)
            print(f"ğŸ‰ æ‰€æœ‰æ‰¹æ¬¡å¤„ç†å®Œæˆï¼")
            print(f"   æ€»å»ºè®®æ•°: {len(all_suggestions)}")
            print(f"   åˆ†ç±»æ•°: {len(all_categories)}")
            print("="*80 + "\n")

            return {
                'suggestions': all_suggestions,
                'categories': all_categories
            }
        else:
            # æ–‡ä»¶æ•°é‡ä¸å¤šï¼Œç›´æ¥å¤„ç†
            print(f"\nğŸ“‹ å•æ‰¹å¤„ç†æ¨¡å¼ - å…± {total_files} ä¸ªæ–‡ä»¶\n")
            result = self._analyze_batch(files, 1, 1)

            # è°ƒç”¨è¿›åº¦å›è°ƒ
            if progress_callback:
                progress_callback(1, 1, result)

            return result

    def _analyze_batch(self, files: List[Dict], batch_num: int = 1, total_batches: int = 1) -> Dict:
        """
        åˆ†æä¸€æ‰¹æ–‡ä»¶ï¼ˆå†…éƒ¨æ–¹æ³•ï¼‰

        :param files: æ–‡ä»¶åˆ—è¡¨
        :param batch_num: å½“å‰æ‰¹æ¬¡å·
        :param total_batches: æ€»æ‰¹æ¬¡æ•°
        """
        print(f"ğŸ”„ å¼€å§‹åˆ†ææ‰¹æ¬¡ {batch_num}/{total_batches}...")

        # æ„é€ å‘é€ç»™AIçš„æç¤ºè¯
        prompt = self._build_prompt(files)

        # è°ƒç”¨é€šä¹‰å¤§æ¨¡å‹API
        try:
            response_text = self._call_tongyi_api(prompt)
            result = self._parse_response(response_text)

            # è§£ææˆåŠŸæ—¥å¿—
            suggestions_count = len(result.get('suggestions', []))
            print(f"âœ… æ‰¹æ¬¡ {batch_num} è§£ææˆåŠŸï¼Œè·å¾— {suggestions_count} æ¡å»ºè®®")

            return result
        except Exception as e:
            print(f"âŒ æ‰¹æ¬¡ {batch_num} AIåˆ†æå¤±è´¥: {e}")
            return self._get_empty_result()

    def _build_prompt(self, files: List[Dict]) -> str:
        """
        æ„å»ºå‘é€ç»™AIçš„æç¤ºè¯
        """
        # é™åˆ¶æ–‡ä»¶æ•°é‡ï¼Œé¿å…è¶…è¿‡tokené™åˆ¶
        if len(files) > config.MAX_FILES_PER_REQUEST:
            files = files[:config.MAX_FILES_PER_REQUEST]

        # æ„é€ æ–‡ä»¶åˆ—è¡¨çš„æè¿°
        files_description = "æ–‡ä»¶åˆ—è¡¨:\n"
        for i, file in enumerate(files, 1):
            files_description += f"{i}. {file['name']} - {file['size_mb']}MB - ä¿®æ”¹æ—¶é—´:{file['modified_time']}\n"
            files_description += f"   è·¯å¾„: {file['path']}\n"

        prompt = f"""ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½æ–‡ä»¶ç®¡ç†åŠ©æ‰‹ã€‚è¯·åˆ†æä»¥ä¸‹æ¡Œé¢å’Œä¸‹è½½æ–‡ä»¶å¤¹ä¸­çš„æ–‡ä»¶ï¼Œå¹¶ç»™å‡ºæ•´ç†å»ºè®®ã€‚

{files_description}

è¯·æŒ‰ç…§ä»¥ä¸‹JSONæ ¼å¼è¿”å›åˆ†æç»“æœï¼ˆåªè¿”å›JSONï¼Œä¸è¦å…¶ä»–æ–‡å­—ï¼‰ï¼š

{{
    "suggestions": [
        {{
            "file_path": "æ–‡ä»¶å®Œæ•´è·¯å¾„",
            "action": "delete/move/keep",
            "reason": "å»ºè®®ç†ç”±",
            "category": "æ–‡ä»¶åˆ†ç±»ï¼ˆå¦‚ï¼šä¸´æ—¶æ–‡ä»¶ã€é‡è¦æ–‡æ¡£ã€å›¾ç‰‡ç­‰ï¼‰",
            "confidence": 0.9
        }}
    ],
    "categories": {{
        "ä¸´æ—¶æ–‡ä»¶": ["æ–‡ä»¶å1", "æ–‡ä»¶å2"],
        "é‡è¦æ–‡æ¡£": ["æ–‡ä»¶å3"]
    }}
}}

åˆ†æè¦ç‚¹ï¼š
1. è¯†åˆ«ä¸´æ—¶æ–‡ä»¶ã€é‡å¤æ–‡ä»¶ã€è¿‡æœŸæ–‡ä»¶ï¼Œå»ºè®®åˆ é™¤
2. å°†æ–‡ä»¶æŒ‰ç±»å‹åˆ†ç±»ï¼ˆæ–‡æ¡£ã€å›¾ç‰‡ã€è§†é¢‘ã€å®‰è£…åŒ…ç­‰ï¼‰
3. æ ‡æ³¨æ¯ä¸ªå»ºè®®çš„ç½®ä¿¡åº¦
4. ç»™å‡ºæ¸…æ™°çš„ç†ç”±

ç°åœ¨è¯·å¼€å§‹åˆ†æã€‚"""

        return prompt

    def _call_tongyi_api(self, prompt: str) -> str:
        """
        è°ƒç”¨é€šä¹‰åƒé—®API

        ã€æ ¸å¿ƒæ–¹æ³• - è¯·åœ¨è¿™é‡Œå®ç°é€šä¹‰å¤§æ¨¡å‹çš„è°ƒç”¨ã€‘

        :param prompt: å‘é€ç»™AIçš„æç¤ºè¯
        :return: AIçš„å“åº”æ–‡æœ¬
        """

        # TODO: åœ¨è¿™é‡Œå®ç°é€šä¹‰åƒé—®APIè°ƒç”¨
        # ====================================
        # æ–¹æ³•1: ä½¿ç”¨ dashscope SDK
        # ====================================
        # import dashscope
        # from dashscope import Generation

        # # è®¾ç½®API Keyï¼ˆé‡è¦ï¼ï¼‰
        # dashscope.api_key = self.api_key

        # response = Generation.call(
        #     model=self.model,
        #     prompt=prompt,
        #     result_format='message'  # æˆ– 'text'
        # )
        
        # if response.status_code == 200:
        #     return response.output.text
        # else:
        #     raise Exception(f"APIè°ƒç”¨å¤±è´¥: {response.message}")

        # ====================================
        # æ–¹æ³•2: ä½¿ç”¨ OpenAI å…¼å®¹çš„ HTTP è¯·æ±‚ï¼ˆå¸¦é‡è¯•æœºåˆ¶ï¼‰
        # ====================================
        import requests
        import time
        import json

        url = "https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions"
        headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json"
        }
        data = {
            "model": self.model,
            "messages": [
                {"role": "user", "content": prompt}
            ]
        }

        # ä½¿ç”¨é…ç½®ä¸­çš„é‡è¯•æ¬¡æ•°
        max_retries = config.AI_MAX_RETRIES

        for attempt in range(max_retries):
            try:
                print(f"\n{'='*80}")
                print(f"ğŸ“¡ é€šä¹‰åƒé—®APIè°ƒç”¨ - å°è¯• {attempt + 1}/{max_retries}")
                print(f"{'='*80}")

                # è®°å½•è¯·æ±‚å‚æ•°
                if config.ENABLE_DETAIL_LOG and config.LOG_REQUEST_PARAMS:
                    print(f"\nğŸ“¤ è¯·æ±‚å‚æ•°:")
                    print(f"   URL: {url}")
                    print(f"   æ¨¡å‹: {self.model}")
                    print(f"   è¶…æ—¶è®¾ç½®: {config.AI_TIMEOUT}ç§’")
                    print(f"   æç¤ºè¯é•¿åº¦: {len(prompt)} å­—ç¬¦")
                    print(f"\n   æ¶ˆæ¯å†…å®¹:")
                    # æˆªå–å‰500å­—ç¬¦é¢„è§ˆ
                    preview = prompt[:500] + "..." if len(prompt) > 500 else prompt
                    print(f"   {preview}")
                    print(f"\n   å®Œæ•´è¯·æ±‚ä½“:")
                    print(f"   {json.dumps(data, ensure_ascii=False, indent=2)[:1000]}...")

                # å‘é€è¯·æ±‚
                start_time = time.time()
                response = requests.post(url, headers=headers, json=data, timeout=config.AI_TIMEOUT)
                elapsed_time = time.time() - start_time

                # æ£€æŸ¥HTTPé”™è¯¯
                response.raise_for_status()

                # è§£æå“åº”
                result = response.json()
                response_content = result['choices'][0]['message']['content']

                # æˆåŠŸæ—¥å¿—
                print(f"\nâœ… APIè°ƒç”¨æˆåŠŸï¼")
                print(f"â±ï¸  è€—æ—¶: {elapsed_time:.2f}ç§’")
                print(f"ğŸ“Š å“åº”é•¿åº¦: {len(response_content)} å­—ç¬¦")

                # è®°å½•å“åº”å†…å®¹
                if config.ENABLE_DETAIL_LOG and config.LOG_RESPONSE_CONTENT:
                    print(f"\nğŸ“¥ å“åº”å†…å®¹:")
                    print(f"   HTTPçŠ¶æ€ç : {response.status_code}")

                    # Tokenä½¿ç”¨æƒ…å†µ
                    if 'usage' in result:
                        usage = result['usage']
                        print(f"\n   Tokenä½¿ç”¨:")
                        print(f"      è¾“å…¥tokens: {usage.get('prompt_tokens', 'N/A')}")
                        print(f"      è¾“å‡ºtokens: {usage.get('completion_tokens', 'N/A')}")
                        print(f"      æ€»è®¡tokens: {usage.get('total_tokens', 'N/A')}")

                    # AIå“åº”å†…å®¹ï¼ˆæˆªå–é¢„è§ˆï¼‰
                    print(f"\n   AIå“åº”å†…å®¹ï¼ˆå‰500å­—ç¬¦ï¼‰:")
                    preview = response_content[:500] + "..." if len(response_content) > 500 else response_content
                    print(f"   {preview}")

                    # å®Œæ•´å“åº”ï¼ˆå¦‚æœä¸å¤ªé•¿ï¼‰
                    if len(response_content) <= 2000:
                        print(f"\n   å®Œæ•´AIå“åº”:")
                        print(f"   {response_content}")

                print(f"{'='*80}\n")

                return response_content

            except requests.exceptions.Timeout as e:
                print(f"\nâš ï¸  è¯·æ±‚è¶…æ—¶ï¼ˆå°è¯• {attempt + 1}/{max_retries}ï¼‰")
                print(f"   é”™è¯¯: {str(e)}")

                if attempt < max_retries - 1:
                    wait_time = config.AI_RETRY_DELAY * (attempt + 1)
                    print(f"   ç­‰å¾… {wait_time} ç§’åé‡è¯•...")
                    time.sleep(wait_time)
                else:
                    raise Exception(f"APIè°ƒç”¨è¶…æ—¶ï¼Œå·²é‡è¯•{max_retries}æ¬¡: {str(e)}")

            except requests.exceptions.HTTPError as e:
                print(f"\nâŒ HTTPé”™è¯¯ï¼ˆå°è¯• {attempt + 1}/{max_retries}ï¼‰")
                print(f"   çŠ¶æ€ç : {response.status_code}")
                print(f"   é”™è¯¯: {str(e)}")
                print(f"   å“åº”å†…å®¹: {response.text[:500]}")

                # æŸäº›é”™è¯¯ä¸éœ€è¦é‡è¯•
                if response.status_code in [401, 403]:
                    raise Exception(f"è®¤è¯é”™è¯¯: {response.text}")

                if attempt < max_retries - 1:
                    wait_time = config.AI_RETRY_DELAY * (attempt + 1)
                    print(f"   ç­‰å¾… {wait_time} ç§’åé‡è¯•...")
                    time.sleep(wait_time)
                else:
                    raise Exception(f"HTTPè¯·æ±‚å¤±è´¥: {str(e)}")

            except requests.exceptions.RequestException as e:
                print(f"\nâŒ ç½‘ç»œè¯·æ±‚é”™è¯¯ï¼ˆå°è¯• {attempt + 1}/{max_retries}ï¼‰")
                print(f"   é”™è¯¯ç±»å‹: {type(e).__name__}")
                print(f"   é”™è¯¯è¯¦æƒ…: {str(e)}")

                if attempt < max_retries - 1:
                    wait_time = config.AI_RETRY_DELAY * (attempt + 1)
                    print(f"   ç­‰å¾… {wait_time} ç§’åé‡è¯•...")
                    time.sleep(wait_time)
                else:
                    raise Exception(f"APIè°ƒç”¨å¤±è´¥: {str(e)}")

            except (KeyError, ValueError, json.JSONDecodeError) as e:
                print(f"\nâŒ å“åº”è§£æé”™è¯¯")
                print(f"   é”™è¯¯ç±»å‹: {type(e).__name__}")
                print(f"   é”™è¯¯è¯¦æƒ…: {str(e)}")
                print(f"   å“åº”æ–‡æœ¬: {response.text[:500] if 'response' in locals() else 'N/A'}")
                raise Exception(f"APIå“åº”æ ¼å¼é”™è¯¯: {str(e)}")

        # ====================================
        # ä¸´æ—¶è¿”å›ï¼ˆç”¨äºæµ‹è¯•ï¼Œè¯·æ›¿æ¢ä¸ºå®é™…çš„APIè°ƒç”¨ï¼‰
        # ====================================
        # print("è­¦å‘Š: AI APIæœªé…ç½®ï¼Œè¿”å›æ¨¡æ‹Ÿæ•°æ®")
        # return self._get_mock_response()

    def _get_mock_response(self) -> str:
        """
        è·å–æ¨¡æ‹Ÿå“åº”ï¼ˆä»…ç”¨äºæµ‹è¯•ï¼‰
        å®é™…ä½¿ç”¨æ—¶ï¼Œè¿™ä¸ªæ–¹æ³•ä¸ä¼šè¢«è°ƒç”¨
        """
        mock_result = {
            "suggestions": [
                {
                    "file_path": "/path/to/example.tmp",
                    "action": "delete",
                    "reason": "è¿™æ˜¯ä¸€ä¸ªä¸´æ—¶æ–‡ä»¶",
                    "category": "ä¸´æ—¶æ–‡ä»¶",
                    "confidence": 0.85
                }
            ],
            "categories": {
                "ä¸´æ—¶æ–‡ä»¶": ["example.tmp"],
                "æ–‡æ¡£": [],
                "å›¾ç‰‡": []
            }
        }
        return json.dumps(mock_result, ensure_ascii=False)

    def _parse_response(self, response_text: str) -> Dict:
        """
        è§£æAIè¿”å›çš„JSONå“åº”
        """
        # å»é™¤å¯èƒ½çš„markdownä»£ç å—æ ‡è®°
        response_text = response_text.strip()
        if response_text.startswith('```json'):
            response_text = response_text[7:]
        if response_text.startswith('```'):
            response_text = response_text[3:]
        if response_text.endswith('```'):
            response_text = response_text[:-3]

        response_text = response_text.strip()

        # è§£æJSON
        result = json.loads(response_text)

        # éªŒè¯æ ¼å¼
        if 'suggestions' not in result:
            result['suggestions'] = []
        if 'categories' not in result:
            result['categories'] = {}

        return result

    def _get_empty_result(self) -> Dict:
        """è¿”å›ç©ºç»“æœ"""
        return {
            'suggestions': [],
            'categories': {}
        }


if __name__ == '__main__':
    # æµ‹è¯•ä»£ç 
    analyzer = AIAnalyzer()

    # æ¨¡æ‹Ÿæ–‡ä»¶åˆ—è¡¨
    test_files = [
        {
            'name': 'temp_file.tmp',
            'path': '/home/user/Desktop/temp_file.tmp',
            'extension': '.tmp',
            'size_mb': 1.5,
            'modified_time': '2024-01-15 10:30:00'
        },
        {
            'name': 'important_doc.pdf',
            'path': '/home/user/Desktop/important_doc.pdf',
            'extension': '.pdf',
            'size_mb': 5.2,
            'modified_time': '2025-11-10 14:20:00'
        }
    ]

    result = analyzer.analyze_files(test_files)
    print("AIåˆ†æç»“æœ:")
    print(json.dumps(result, indent=2, ensure_ascii=False))
